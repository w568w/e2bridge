é¡¹ç›® 'enginelabs-2api' çš„ç»“æž„æ ‘:
ðŸ“‚ enginelabs-2api/
    ðŸ“„ .env
    ðŸ“„ .env.example
    ðŸ“„ Dockerfile
    ðŸ“„ docker-compose.yml
    ðŸ“„ main.py
    ðŸ“„ nginx.conf
    ðŸ“„ requirements.txt
    ðŸ“‚ app/
        ðŸ“‚ core/
            ðŸ“„ __init__.py
            ðŸ“„ config.py
        ðŸ“‚ providers/
            ðŸ“„ __init__.py
            ðŸ“„ base_provider.py
            ðŸ“„ enginelabs_provider.py
        ðŸ“‚ utils/
            ðŸ“„ sse_utils.py
================================================================================

--- æ–‡ä»¶è·¯å¾„: .env ---

# [æœ€ç»ˆç”Ÿäº§é…ç½® v5.0]
# API_MASTER_KEY å·²è®¾ç½®ã€‚
# CLERK_COOKIE å·²ä»Žæ‚¨çš„åŽŸå§‹æŠ“åŒ…æ•°æ®ä¸­è‡ªåŠ¨å¡«å……ã€‚

# --- æ ¸å¿ƒå®‰å…¨é…ç½® ---
API_MASTER_KEY=1

# --- éƒ¨ç½²é…ç½® ---
NGINX_PORT=8089

# --- Clerk è®¤è¯å‡­è¯ ---
CLERK_COOKIE="_cfuvid=dPYSpA0MmuEfGfqYRM035h98ti.6eeEzvWsMxTc7ucQ-1760709424362-0.0.1.1-604800000; _ga=GA1.1.978033660.1760709426; _hjSession_3492377=eyJpZCI6ImM1Yzg3YWY2LTkxMDQtNDVhOS04ZDIzLTk3YmFiZDA2YjcwZiIsImMiOjE3NjA3MDk0MjYzMzYsInMiOjAsInIiOjAsInNiIjowLCJzciI6MCwic2UiOjAsImZzIjoxLCJzcCI6MH0=; __cf_bm=VI57AHhk59VNP5SXo8ysWUmGAUttoVNId6dvMG3HuOk-1760709453-1.0.1.1-v0KXcL6iH4.RGl1OcjAapluWFxzUIl2GIlBRZmkxnX2o7ZWm0uJ9QSSgKrvn66NKLvvU8Oq0sD57E3zjg.jeS1MgxTjgReONPi4PiTAWEbpif77b7z6kgKRcebE5urIM; __client=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6ImNsaWVudF8zNENGMGxVUENlQjNoT2ZjOVhDSFdETk9lVXoiLCJyb3RhdGluZ190b2tlbiI6Im5kaHZ3bGo2ZHVqenJhMzUybzF1MTRtaG12NXhiYmE3c2doeG1zMHkifQ.WLfPgTp_EtMQVgQith_4-HVcR4E0YOAaJALY7e4yuRTByoat2_RJsJ3Z1-QMK30VggOk6_nYd_6GlQL6FkcmvEYPS1zKPiOv7eqoQC931_J3c3L3w4l8rG1wSCcNEpSfDRPHnAYnySSTrxlt_uXoBLxzE_jBpZTdd4MgpBQeAXoHU0DK-oRTu_K420SlXYQotGmNUPlwB18We_jxgEWjIFajXNS6HR_EfGKmCuXaktZJBjPkbAjixlTtCiE9lopmLA2mCay78fqzd_TC5SgJaYzIRSdhneVPfjNlww4SmGnF5YbFrsNADrzGJuAe-dieaKrjYUw4a-UPZZRMO6dIow; _hjSessionUser_3492377=eyJpZCI6IjY1YzdmNjRlLTM1YmUtNWJmZS04ZTQxLTZhOTIyMmM5MzA4MyIsImNyZWF0ZWQiOjE3NjA3MDk0MjYzMzUsImV4aXN0aW5nIjp0cnVlfQ==; __client_uat=1760709474; __client_uat_LUZhIdZf=1760709474; _ga_YXQL57VHZ5=GS2.1.s1760709425$o1$g1$t1760710084$j43$l0$h0; _ga_H9LTP678KS=GS2.1.s1760709425$o1$g1$t1760710084$j43$l0$h0; _gcl_au=1.1.1896293932.1760709426.1309957906.1760709967.1760710084"


--- æ–‡ä»¶è·¯å¾„: .env.example ---

# ====================================================================
# enginelabs-2api é…ç½®æ–‡ä»¶æ¨¡æ¿ (v4.0 - è‡ªä¸»è®¤è¯ç‰ˆ)
# ====================================================================

# --- æ ¸å¿ƒå®‰å…¨é…ç½® (å¿…é¡»è®¾ç½®) ---
# ç”¨äºŽä¿æŠ¤æ‚¨ API æœåŠ¡çš„è®¿é—®å¯†é’¥ã€‚
API_MASTER_KEY=lzA6_enginelabs_2api_key

# --- éƒ¨ç½²é…ç½® (å¯é€‰) ---
# Nginx å¯¹å¤–æš´éœ²çš„ç«¯å£
NGINX_PORT=8089

# --- Clerk è®¤è¯å‡­è¯ (å¿…é¡»è®¾ç½®) ---
# è¿™æ˜¯å®žçŽ°è‡ªåŠ¨èŽ·å–ä»¤ç‰Œçš„å…³é”®ã€‚è¯·ä»Žæµè§ˆå™¨å¼€å‘è€…å·¥å…·ä¸­èŽ·å–ã€‚
# è¯¦ç»†æ­¥éª¤è¯·å‚è€ƒ README.mdã€‚
CLERK_COOKIE="eyJhbGciOiJSUzI1NiIsImNhdCI6ImNsX0I3ZDRQRDExMUFBQSIsImtpZCI6Imluc18yUkhSa1NDcVVUUEowbzlUU3FUdnA2OFlmdlQiLCJ0eXAiOiJKV1QifQ.eyJhenAiOiJodHRwczovL2N0by5uZXciLCJleHAiOjE3NjA3MTA3MzAsImZ2YSI6WzE5LC0xXSwiaWF0IjoxNzYwNzEwNjcwLCJpc3MiOiJodHRwczovL2NsZXJrLmN0by5uZXciLCJuYmYiOjE3NjA3MTA2NjAsIm8iOnsiaWQiOiJvcmdfMzRDRzBUR0VZSFlQUFVFVkM2eGlZN3FBRlVPIiwicm9sIjoiYWRtaW4iLCJzbGciOiJsemE2LTE3NjA3MDk5MTYifSwic2lkIjoic2Vzc18zNENGNnJ4Z0hydmJvQ2lybTNrNXNWSkwyTEYiLCJzdHMiOiJhY3RpdmUiLCJzdWIiOiJ1c2VyXzM0Q0Y2djBnYmw1V292ck5xZ09aeFBBbmc2OSIsInYiOjJ9.F0x_F1N9g4pjxTswo7pRCIyWptZ7C4dXFemAeErZy8mgUFDqRGaGYlYGTTz0NT7geQ1OxDnIVVu9Nqyptt6PPn1hUG_wDwE90C-jfhmKFr2iG_ezyrR7IvtmRhm788K2-fmMYkgIdUoyIBk4HtW7KeErpWEmg4SoVb59GghMd_S7cFSRBARZDCDh1gsEjeS3IMF5zzElEUrAnxWYf9rFHFe5T63je-1azgM3NabSOIzS7zUYImtrB63Csdk8Ib5LfVLtkjQ9Rv4z7GwtfsgUMNQ--tVVMYAGtZA50RQ_fzc7NmDqOwW_j12HHchydYaQ1DF6SCzjjZm9GcaVSTRTQg"


--- æ–‡ä»¶è·¯å¾„: Dockerfile ---

# ====================================================================
# Dockerfile for wald-2api
# ====================================================================

FROM python:3.10-slim

# è®¾ç½®çŽ¯å¢ƒå˜é‡
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
WORKDIR /app

# å®‰è£… Python ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¹¶åˆ‡æ¢åˆ°éž root ç”¨æˆ·
RUN useradd --create-home appuser && \
    chown -R appuser:appuser /app
USER appuser

# æš´éœ²ç«¯å£å¹¶å¯åŠ¨
EXPOSE 8000
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]


--- æ–‡ä»¶è·¯å¾„: docker-compose.yml ---

services:
  nginx:
    image: nginx:latest
    container_name: enginelabs-2api-nginx
    restart: always
    ports:
      - "${NGINX_PORT:-8089}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - app
    networks:
      - enginelabs-net

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: enginelabs-2api-app
    restart: unless-stopped
    env_file:
      - .env
    networks:
      - enginelabs-net

networks:
  enginelabs-net:
    driver: bridge


--- æ–‡ä»¶è·¯å¾„: main.py ---

import logging
from contextlib import asynccontextmanager
from typing import Optional

from fastapi import FastAPI, Request, HTTPException, Depends, Header
from fastapi.responses import JSONResponse, StreamingResponse

from app.core.config import settings
from app.providers.enginelabs_provider import EngineLabsProvider

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å…¨å±€ Provider å®žä¾‹ ---
provider = EngineLabsProvider()

# --- FastAPI Lifespan ç®¡ç† ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"å¯åŠ¨åº”ç”¨: {settings.APP_NAME} v{settings.APP_VERSION}")
    logger.info("åè®®: å‡¤å‡°åè®® (Project Phoenix) | v4.0 è‡ªä¸»è®¤è¯ç‰ˆ")
    logger.info("è®¤è¯æ¨¡å¼: å…¨è‡ªåŠ¨ä»¤ç‰Œç»­æœŸ")
    logger.info(f"æœåŠ¡å°†åœ¨ http://localhost:{settings.NGINX_PORT} ä¸Šå¯ç”¨")
    yield
    logger.info("åº”ç”¨å…³é—­ã€‚")

# --- FastAPI åº”ç”¨å®žä¾‹ ---
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description=settings.DESCRIPTION,
    lifespan=lifespan
)

# --- ä¾èµ–é¡¹ï¼šAPI Key éªŒè¯ ---
async def verify_api_key(authorization: Optional[str] = Header(None)):
    if settings.API_MASTER_KEY:
        if not authorization or "bearer" not in authorization.lower():
            raise HTTPException(status_code=401, detail="éœ€è¦æä¾› Master API Key è¿›è¡Œè®¤è¯ (Bearer Token)ã€‚")
        token = authorization.split(" ")[-1]
        if token != settings.API_MASTER_KEY:
            raise HTTPException(status_code=403, detail="æ— æ•ˆçš„ Master API Keyã€‚")

# --- API è·¯ç”± ---
@app.post("/v1/chat/completions", dependencies=[Depends(verify_api_key)])
async def chat_completions(request: Request):
    """
    å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚ã€‚è®¤è¯ä¿¡æ¯ç”±æœåŠ¡å…¨è‡ªåŠ¨å¤„ç†ã€‚
    """
    try:
        request_data = await request.json()
        # ã€è°ƒç”¨ç®€åŒ–ã€‘: ä¸å†éœ€è¦ä¼ é€’ä»»ä½•ä»¤ç‰Œ
        return await provider.chat_completion(request_data)

    except HTTPException as e:
        raise e
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‘ç”Ÿé¡¶å±‚é”™è¯¯: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"error": {"message": f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(e)}", "type": "internal_server_error"}})

@app.get("/v1/models", dependencies=[Depends(verify_api_key)], response_class=JSONResponse)
async def list_models():
    return await provider.get_models()

@app.get("/", summary="æ ¹è·¯å¾„", include_in_schema=False)
def root():
    return {"message": f"æ¬¢è¿Žæ¥åˆ° {settings.APP_NAME} v{settings.APP_VERSION}. æœåŠ¡è¿è¡Œæ­£å¸¸ã€‚"}


--- æ–‡ä»¶è·¯å¾„: nginx.conf ---

worker_processes auto;

events {
    worker_connections 1024;
}

http {
    upstream enginelabs_backend {
        # ip_hash ä¿è¯åŒä¸€å®¢æˆ·ç«¯çš„å¤šä¸ªè¯·æ±‚ï¼ˆå¦‚æžœå‘ç”Ÿï¼‰è¢«è·¯ç”±åˆ°åŒä¸€ workerï¼Œ
        # è™½ç„¶å¯¹äºŽæ— çŠ¶æ€ API ä¸æ˜¯å¿…é¡»ï¼Œä½†ä½œä¸ºè‰¯å¥½å®žè·µä¿ç•™ã€‚
        ip_hash;
        server app:8000;
    }

    server {
        listen 80;
        server_name localhost;

        location / {
            proxy_pass http://enginelabs_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # --- æµå¼ä¼ è¾“ä¼˜åŒ– ---
            proxy_buffering off;
            proxy_cache off;
            proxy_set_header Connection '';
            proxy_http_version 1.1;
            chunked_transfer_encoding off;
        }
    }
}


--- æ–‡ä»¶è·¯å¾„: requirements.txt ---

fastapi
uvicorn[standard]
pydantic-settings
python-dotenv
cloudscraper
websockets
PyJWT


--- æ–‡ä»¶è·¯å¾„: app\core\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\core\config.py ---

from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import List, Optional

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding='utf-8',
        extra="ignore"
    )

    APP_NAME: str = "enginelabs-2api"
    APP_VERSION: str = "4.0.0-Phoenix"
    DESCRIPTION: str = "ä¸€ä¸ªå°† cto.new (EngineLabs) API è½¬æ¢ä¸ºå…¼å®¹ OpenAI æ ¼å¼çš„é«˜æ€§èƒ½ä»£ç†ã€‚å®žçŽ°å…¨è‡ªåŠ¨ä»¤ç‰Œç»­æœŸï¼Œä¸€åŠ³æ°¸é€¸ã€‚"

    # --- å®‰å…¨ä¸Žç½‘ç»œ ---
    API_MASTER_KEY: Optional[str] = None
    NGINX_PORT: int = 8089
    API_REQUEST_TIMEOUT: int = 300

    # --- Clerk è®¤è¯å‡­è¯ ---
    CLERK_COOKIE: Optional[str] = None

    # --- æ¨¡åž‹é…ç½® ---
    DEFAULT_MODEL: str = "ClaudeSonnet4_5"
    KNOWN_MODELS: List[str] = ["ClaudeSonnet4_5", "GPT5"]

settings = Settings()


--- æ–‡ä»¶è·¯å¾„: app\providers\__init__.py ---



--- æ–‡ä»¶è·¯å¾„: app\providers\base_provider.py ---

from abc import ABC, abstractmethod
from typing import Dict, Any
from fastapi.responses import StreamingResponse, JSONResponse

class BaseProvider(ABC):
    @abstractmethod
    async def chat_completion(
        self,
        request_data: Dict[str, Any]
    ) -> StreamingResponse:
        pass

    @abstractmethod
    async def get_models(self) -> JSONResponse:
        pass


--- æ–‡ä»¶è·¯å¾„: app\providers\enginelabs_provider.py ---

import json
import time
import logging
import uuid
import jwt
import cloudscraper
import websockets
import hashlib
from typing import Dict, Any, AsyncGenerator, Optional

from fastapi import HTTPException
from fastapi.responses import StreamingResponse, JSONResponse

from app.core.config import settings
from app.providers.base_provider import BaseProvider
from app.utils.sse_utils import create_sse_data, create_chat_completion_chunk, DONE_CHUNK

logger = logging.getLogger(__name__)

class EngineLabsProvider(BaseProvider):
    """
    Provider for EngineLabs API.
    v10.0: The Final Truth. This version combines all successful learnings:
    - Autonomous JWT refresh using cookies.
    - Server-side context memory using a message hash cache.
    - Reverted to the simple 'prompt' string format, which is the only one accepted by the server.
    This is the definitive, stable, and fully functional version.
    """
    def __init__(self):
        if not settings.CLERK_COOKIE:
            raise ValueError("é…ç½®é”™è¯¯: CLERK_COOKIE å¿…é¡»åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®ã€‚")
        
        self.scraper = cloudscraper.create_scraper()
        self.chat_url = "https://api.enginelabs.ai/engine-agent/chat"
        
        self.clerk_cookie = settings.CLERK_COOKIE.strip()
        
        self.session_id = "sess_34CF6rxgHrvboCirm3k5sVJL2LF"
        self.organization_id = "org_34CG0TGEYHYPPUEVC6xiY7qAFUO"
        
        self.token_url = f"https://clerk.cto.new/v1/client/sessions/{self.session_id}/tokens?__clerk_api_version=2025-04-10"
        
        self.conversation_cache: Dict[str, str] = {}
        
        logger.info("EngineLabsProvider å·²åˆå§‹åŒ– (çœŸç†åè®® v10.0)ï¼ŒæœåŠ¡å·²å°±ç»ªã€‚")

    async def _get_fresh_jwt(self) -> str:
        logger.info("æ­£åœ¨è¯·æ±‚æ–°çš„ JWT ä»¤ç‰Œ...")
        headers = {
            "Cookie": self.clerk_cookie,
            "Content-Type": "application/x-www-form-urlencoded",
            "Origin": "https://cto.new",
            "Referer": "https://cto.new/",
        }
        form_data = { "organization_id": self.organization_id }
        try:
            response = self.scraper.post(self.token_url, headers=headers, data=form_data)
            response.raise_for_status()
            data = response.json()
            new_jwt = data.get("jwt")
            if not new_jwt: raise ValueError("å“åº”ä¸­ç¼ºå°‘ 'jwt' å­—æ®µã€‚")
            logger.info("æˆåŠŸèŽ·å–æ–°çš„ JWT ä»¤ç‰Œã€‚")
            return new_jwt
        except Exception as e:
            logger.error(f"èŽ·å–æ–° JWT ä»¤ç‰Œå¤±è´¥: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail=f"æ— æ³•ä»Ž Clerk è‡ªåŠ¨èŽ·å–è®¤è¯ä»¤ç‰Œ: {e}")

    def _get_conversation_fingerprint(self, messages: list) -> str:
        if not messages:
            return "empty"
        history_str = json.dumps(messages, sort_keys=True)
        return hashlib.md5(history_str.encode('utf-8')).hexdigest()

    async def chat_completion(self, request_data: Dict[str, Any]) -> StreamingResponse:
        
        jwt_token = await self._get_fresh_jwt()
        user_id = jwt.decode(jwt_token, options={"verify_signature": False}).get("sub")
        
        model = request_data.get("model", settings.DEFAULT_MODEL)
        messages = request_data.get("messages", [])
        
        history_messages = messages[:-1]
        fingerprint = self._get_conversation_fingerprint(history_messages)
        
        if fingerprint in self.conversation_cache:
            chat_history_id = self.conversation_cache[fingerprint]
            logger.info(f"åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°å¯¹è¯æŒ‡çº¹ '{fingerprint}'ï¼Œå¤ç”¨ chatHistoryId: {chat_history_id}")
        else:
            chat_history_id = str(uuid.uuid4())
            self.conversation_cache[fingerprint] = chat_history_id
            logger.info(f"æœªæ‰¾åˆ°å¯¹è¯æŒ‡çº¹ '{fingerprint}'ï¼Œåˆ›å»ºæ–° chatHistoryId: {chat_history_id}")

        async def stream_generator() -> AsyncGenerator[bytes, None]:
            request_id = f"chatcmpl-{uuid.uuid4()}"
            websocket_uri = f"wss://api.enginelabs.ai/engine-agent/chat-histories/{chat_history_id}/buffer/stream?token={user_id}"
            
            try:
                last_user_message = messages[-1]["content"]
                
                headers = {
                    "Authorization": f"Bearer {jwt_token}",
                    "Content-Type": "application/json",
                    "Origin": "https://cto.new",
                    "Referer": f"https://cto.new/{chat_history_id}",
                }
                
                # ã€ã€ã€ æœ€ç»ˆã€çœŸæ­£çš„æ ¸å¿ƒä¿®æ­£ ã€‘ã€‘ã€‘
                # å›žå½’åˆ°è¢«è¯æ˜Žæ˜¯æ­£ç¡®çš„ã€ç®€å•çš„ payload ç»“æž„
                payload = {
                    "prompt": last_user_message, # prompt å¿…é¡»æ˜¯ç®€å•çš„å­—ç¬¦ä¸²
                    "chatHistoryId": chat_history_id,
                    "adapterName": model,
                }
                
                logger.info(f"å‘é€ POST è¯·æ±‚ï¼Œä½¿ç”¨ chatHistoryId: {chat_history_id}")
                trigger_response = self.scraper.post(self.chat_url, headers=headers, json=payload, allow_redirects=False)
                
                trigger_response.raise_for_status()
                logger.info(f"POST è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {trigger_response.status_code}")

                logger.info(f"æ­£åœ¨è¿žæŽ¥åˆ° WebSocket: {websocket_uri}")
                async with websockets.connect(websocket_uri, origin="https://cto.new") as websocket:
                    logger.info("WebSocket è¿žæŽ¥æˆåŠŸï¼Œå¼€å§‹ç›‘å¬ä¸Šæ¸¸æ•°æ®...")
                    
                    initial_chunk = create_chat_completion_chunk(request_id, model, "")
                    yield create_sse_data(initial_chunk)

                    while True:
                        message = await websocket.recv()
                        logger.info(f"æ”¶åˆ° WebSocket åŽŸå§‹æ¶ˆæ¯: {message}")
                        
                        data = json.loads(message)
                        
                        if data.get("type") == "state" and not data.get("state", {}).get("inProgress"):
                            logger.info("æ£€æµ‹åˆ°ä¸Šæ¸¸ä»»åŠ¡ç»“æŸä¿¡å·ï¼Œä¸»åŠ¨å…³é—­æµã€‚")
                            break

                        if data.get("type") == "update":
                            buffer_str = data.get("buffer", "{}")
                            try:
                                buffer_data = json.loads(buffer_str)
                                if buffer_data.get("type") == "chat":
                                    content = buffer_data.get("chat", {}).get("content")
                                    if content:
                                        logger.info(f"è§£æžå¹¶å‘é€å†…å®¹: {content}")
                                        chunk = create_chat_completion_chunk(request_id, model, content)
                                        yield create_sse_data(chunk)
                            except json.JSONDecodeError:
                                logger.warning(f"æ— æ³•è§£æž WebSocket buffer: {buffer_str}")
                                continue
            
            except Exception as e:
                logger.error(f"å¤„ç†æµæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
                error_message = f"å†…éƒ¨é”™è¯¯: {str(e)}"
                error_chunk = create_chat_completion_chunk(request_id, model, error_message, "stop")
                yield create_sse_data(error_chunk)
            finally:
                final_chunk = create_chat_completion_chunk(request_id, model, "", "stop")
                yield create_sse_data(final_chunk)
                logger.info("SSE æµå·²å‘é€ [DONE] æ ‡å¿—ï¼Œè¯·æ±‚å¤„ç†å®Œæ¯•ã€‚")

        return StreamingResponse(stream_generator(), media_type="text/event-stream")

    async def get_models(self) -> JSONResponse:
        model_data = {
            "object": "list",
            "data": [
                {"id": name, "object": "model", "created": int(time.time()), "owned_by": "lzA6"}
                for name in settings.KNOWN_MODELS
            ]
        }
        return JSONResponse(content=model_data)


--- æ–‡ä»¶è·¯å¾„: app\utils\sse_utils.py ---

import json
import time
from typing import Dict, Any, Optional

DONE_CHUNK = b"data: [DONE]\n\n"

def create_sse_data(data: Dict[str, Any]) -> bytes:
    return f"data: {json.dumps(data)}\n\n".encode('utf-8')

def create_chat_completion_chunk(
    request_id: str,
    model: str,
    content: str,
    finish_reason: Optional[str] = None
) -> Dict[str, Any]:
    return {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [
            {
                "index": 0,
                "delta": {"content": content},
                "finish_reason": finish_reason,
                "logprobs": None
            }
        ],
        "system_fingerprint": "fp_wald_2api"
    }



